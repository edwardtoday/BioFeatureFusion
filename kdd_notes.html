<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="stylesheets/styles.css">
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="notes-on-knowledge-discovery-from-data-kdd">Notes on Knowledge Discovery from Data (KDD)</h1>
<p>by Pei QING <script type="text/javascript">
<!--
h='&#x63;&#x6f;&#x6d;&#112;&#46;&#112;&#x6f;&#108;&#x79;&#x75;&#46;&#x65;&#100;&#x75;&#46;&#104;&#x6b;';a='&#64;';n='&#x63;&#x73;&#112;&#x71;&#x69;&#110;&#x67;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>&#x63;&#x73;&#112;&#x71;&#x69;&#110;&#x67;&#32;&#x61;&#116;&#32;&#x63;&#x6f;&#x6d;&#112;&#32;&#100;&#x6f;&#116;&#32;&#112;&#x6f;&#108;&#x79;&#x75;&#32;&#100;&#x6f;&#116;&#32;&#x65;&#100;&#x75;&#32;&#100;&#x6f;&#116;&#32;&#104;&#x6b;</noscript></p>
<h1 id="what-is-kdd">What Is KDD?</h1>
<ul>
<li>KDD is a process that attempts to discover <strong>patterns</strong> in large data sets</li>
<li>KDD's overall goal is to extract information from a data set and transform it into an <strong>understandable structure</strong> for further use, e.g. a decision support system.</li>
</ul>
<p>KDD process can be shown as an iterative sequence of the following steps <span class="citation" data-cites="Han:2011wk">[1]</span>:</p>
<ol type="1">
<li><a href="#data-cleaning"><strong>Data cleaning</strong></a> (to remove noise and inconsistent data)</li>
<li><a href="#data-integration"><strong>Data integration</strong></a> (where multiple data sources are combined)</li>
<li><strong>Data selection</strong> (where data relevant to the analysis task are retrieved from the database)</li>
<li><a href="#data-transformation-and-data-discretization"><strong>Data transformation</strong></a> (where data are transformed and consolidated into forms appropriate for mining by performing summary or aggregation operations)<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup></li>
<li><strong>Data mining</strong> (an essential process where intelligent methods are applied to extract data patterns)</li>
<li><strong>Pattern evaluation</strong> (to identify the truly <a href="#which-patterns-are-interesting">interesting patterns</a> representing knowledge based on <em>interestingness measures</em>)</li>
<li><strong>Knowledge presentation</strong> (where visualization and knowledge representation techniques are used to present mined knowledge to users)</li>
</ol>
<h1 id="what-kinds-of-patterns-can-be-mined">What Kinds of Patterns Can Be Mined?</h1>
<h2 id="classconcept-description-characterization-and-discrimination">Class/Concept Description: Characterization and Discrimination</h2>
<p><strong>Data characterization</strong> is a summarization of the general characteristics or features of a target class of data.</p>
<ul>
<li>Statistical measures</li>
<li>OLAP</li>
<li>Attribute-oriented Induction</li>
</ul>
<p><strong>Data discrimination</strong> is a comparison of the general features of the target class data objects against the general features of objects from one or multiple contrasting classes.</p>
<h2 id="mining-frequent-patterns-associations-and-correlations">Mining Frequent Patterns, Associations, and Correlations</h2>
<p><strong>Frequent patterns</strong>: Itemsets, subsequences or sub structures that appear frequently in the data set.</p>
<p>Support &amp; Confidence</p>
<p><span class="math">\[support(A⇒B) = P(A∪B)\]</span></p>
<p><span class="math">\[confidence(A⇒B) = P(B|A)\]</span></p>
<p>Association rule</p>
<p><span class="math">\[computer ⇒ antivirus\ software [support = 2\%, confidence = 60\%]\]</span></p>
<p><strong>Strong</strong> association rule: rules that satisfy both a minimum support threshold (<em>min_sup</em>) and a minimum confidence threshold (<em>min_conf</em>).</p>
<p>Association rule mining</p>
<ol type="1">
<li>Find all frequent item sets with at least <em>min_sup</em>.</li>
</ol>
<ul>
<li>The number of frequent itemsets is too large for a large data set. If we have a frequent k-itemset, the total number of frequent itemsets that it contains is <span class="math">\(2^k - 1\)</span>.</li>
<li>Finding closed &amp; maximal frequent itemsets
<ul>
<li><strong>closed frequent itemset</strong>: no frequent super-itemset with same support</li>
<li><strong>maximal frequent itemset</strong>: no frequent super-itemset</li>
<li>item merging</li>
<li>sub-itemset pruning</li>
<li>item skipping</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>Generate strong association rules from the frequent itemsets.</li>
</ol>
<p>Correlation rule</p>
<p><span class="math">\[A ⇒ B [support, confidence, correlation]\]</span></p>
<h3 id="algorithms">Algorithms</h3>
<ul>
<li><strong>Apriori</strong> <span class="citation" data-cites="Agrawal:1994ut">[2]</span>: Finding frequent itemsets by confined candidate generation</li>
</ul>
<ol type="1">
<li>Hashing itemsets into buckets [PCY95a]</li>
<li>transaction reduction</li>
<li>partitioning the data to find candidate itemsets [SON95]</li>
<li>sampling: mining on a subset of the given data [Toi96]</li>
<li>dynamic itemset counting: start with count-so-far; fewer database scans [BMUT97]</li>
<li>parallel and distributed association mining [PCY95b][AS96] [CHN+96][ZPOL97]</li>
</ol>
<ul>
<li><strong>A-Close</strong> [PBTL99]: Finding frequent closed itemsets</li>
<li><strong>FPgrowth</strong> [HPY00]: Pattern-growth approach for mining frequent itemsets</li>
<li><strong>CLOSET</strong> [PHM00]: Closed itemset mining based on FPgrowth</li>
<li><strong>Eclat</strong> (Equivalence Class Transformation) [Zak00]: mining frequent itemsets using the vertical data format</li>
</ul>
<h2 id="classification-and-regression-for-predictive-analysis">Classification and Regression for Predictive Analysis</h2>
<p><strong>Classification</strong> is the process of finding a <strong>model</strong> (or function) that describes and distinguishes data classes or concepts. The model are derived based on the analysis of a set of <strong>training data</strong> (i.e., data objects for which the class labels are known). The model is used to predict the class label of objects for which the the class label is unknown.</p>
<p>Model representation forms:</p>
<ul>
<li><strong>classification rules</strong> (i.e. IF-THEN rules)</li>
<li><a href="#decision-tree"><strong>decision tree</strong></a></li>
<li><strong>neural network</strong></li>
</ul>
<p>Whereas classification predicts <em>categorical</em> (discrete, unordered) labels, <strong>regression</strong> models <em>continuous-valued</em> functions.</p>
<p><strong>Regression analysis</strong> is a statistical methodology that is most often used for numeric prediction, although other methods exist as well. Regression also encompasses the identification of distribution <em>trends</em> based on the available data.</p>
<h3 id="decision-tree">Decision Tree</h3>
<p>A <strong>decision tree</strong> is a flowchart-like tree structure, where each <strong>internal node</strong> (non- leaf node) denotes a test on an attribute, each <strong>branch</strong> represents an outcome of the test, and each <strong>leaf node</strong> (or terminal node) holds a class label.</p>
<h4 id="algorithms-to-generate-decision-trees">Algorithms to Generate Decision Trees</h4>
<ul>
<li><strong>ID3</strong></li>
<li><strong>C4.5</strong></li>
<li><strong>CART</strong></li>
<li>CHAID, popular in marketing</li>
</ul>
<p>At each level, select the attribute in the <strong>attribute list</strong> that, according to the <strong>attribute selection method</strong>, &quot;best&quot; discriminates the given tuples. If no attribute is available at current level, return the majority class.</p>
<p>ID3 uses <strong>information gain</strong> as its attribute selection measure. This is biased toward attributes with a large number of values. (E.g. N items have unique IDs. Selecting ID as the &quot;best&quot; attribute returns N nodes each with only one tuple.)</p>
<p><span class="math">\[Info(D) = -\sum_{i=1}^{m}{p_{i} log_2(p_{i})}\]</span> <span class="math">\[Info_{A}(D) = \sum_{j=1}^{v}{\frac{|D_j|}{|D|}\times Info(D_{j})}\]</span> <span class="math">\[Gain(A) = Info(D) - Info_{A}(D)\]</span></p>
<p>C4.5 uses <strong>gain ratio</strong> to overcome this bias by normalizing information gain.</p>
<p><span class="math">\[SplitInfo_{A}(D) = -\sum_{j=1}^{v}{\frac{|D_j|}{|D|} \times
log_{2}(\frac{|D_j|}{|D|})}\]</span> <span class="math">\[GainRatio(A) = \frac{Gain(A)}{SplitInfo_{A}(D)}\]</span></p>
<p>CART uses <strong>Gini index</strong>.</p>
<p><span class="math">\[Gini(D) = 1 - \sum_{i=1}^{m}{p_{i}^2}\]</span></p>
<p>CHAID uses a measure based on ChiSquare test.</p>
<p>Other attribute selection measures include</p>
<ul>
<li>C-SEP</li>
<li>G-statistic</li>
<li><strong>Minimum Description Length (MDL)</strong>: Encode both the <em>tree</em> and <em>exceptions</em> to the tree. Attribute that requires least bits for encoding is the best. It is least bias toward multivalued attributes.</li>
<li>multivariate splits: partition based on a <em>combination</em> of attributes. CART can find multivariate splits based on a linear combination of attributes. This is a form of <strong>attribute construction.</strong></li>
</ul>
<p>Measures that tend to produce <em>shallower trees</em> may be preferred. (Though shallower trees tend to have a large number of leaves and higher error rates.)</p>
<h4 id="tree-pruning">Tree Pruning</h4>
<p><strong>Tree pruning</strong> is a method to address the <em>overfitting</em> problem.</p>
<ul>
<li>The <strong>cost complexity</strong> pruning algorithm used in CART is an example of the postpruning approach. This approach considers the cost complexity of a tree to be a function of the number of leaves in the tree and the error rate of the tree (where the <strong>error rate</strong> is the percentage of tuples misclassified by the tree).</li>
<li>C4.5 uses <strong>pessimistic pruning</strong> in which it uses training set to estimate error rates.</li>
<li>MDL prunes a tree based on number of bits required to encode it.</li>
</ul>
<h4 id="problems-with-decision-trees">Problems with Decision Trees</h4>
<ul>
<li><strong>Repetition</strong>: The same attribute may be tested repeatedly along a branch. E.g. &quot;price &gt; 50?&quot;, then &quot;price &lt; 100?&quot;, followed by &quot;100 &gt; price &gt; 80&quot;, ...</li>
<li><strong>Replication</strong>: Duplicate subtrees exist within the tree (, possibly in different branches.)</li>
</ul>
<p>Use multivariate splits to relieve these problems. Or use a rule-based classifier instead of a decision tree classifier.</p>
<ul>
<li>Scalability: ID3, C4.5 and CART works for small data sets. What if the data set is disk-resident?
<ul>
<li><strong>RainForest</strong>: maintains an AVC-set (attribute-value, classlabel)</li>
<li><strong>BOAT</strong> (Bootstrapped Optimistic Algorithm for Tree Construction): bootstrapping to create smaller samples of the training data that fits in memory. Each subset is used to construct a tree. The trees are examined and used to construct a new tree that is &quot;very close&quot; to the tree that would have been generated if all training data had fit in memory.</li>
<li>2~3x faster than RainForest while constructing exactly the same tree.</li>
<li>Incremental updates. BOAT can take new insertions and deletions for the training data without having to reconstruct the tree.</li>
</ul></li>
</ul>
<h3 id="bayes-classification">Bayes Classification</h3>
<p><span class="math">\(P(H)\)</span>, <span class="math">\(P(X|H)\)</span> and <span class="math">\(P(X)\)</span> may be estimated from the given data. The posterior probability, <span class="math">\(P(H|X)\)</span>, can be calculated from them. Bayes's theorem:</p>
<p><span class="math">\[P(H|X) = \frac{P(X|H)P(H)}{P(X)}\]</span></p>
<p>Naive Bayesian Classifier</p>
<ul>
<li>assumption: <strong>class-conditional independence</strong></li>
<li>maximize <span class="math">\(P(X|C_i)\)</span></li>
<li>Note: If we have a probability of zero, <strong>Laplacian correction</strong> can help.</li>
</ul>
<h3 id="rule-based-classification">Rule-Based Classification</h3>
<p>An IF-THEN rule:</p>
<blockquote>
<p>IF <em>condition</em> THEN <em>conclusion</em>.</p>
</blockquote>
<p>A rule <span class="math">\(R\)</span> can be assessed by its <strong>coverage</strong> and <strong>accuracy</strong>.</p>
<p><span class="math">\[coverage(R) = \frac{n_{covers}}{|D|}\]</span></p>
<p><span class="math">\[accuracy(R) = \frac{n_{correct}}{n_{covers}}\]</span></p>
<h4 id="extracting-classification-rules-from-a-decision-tree">Extracting Classification Rules From a Decision Tree</h4>
<p>Each leaf forms a rule. The rules are <strong>mutually exclusive</strong> (no conflicted rules) and <strong>exhaustive</strong> (no need for a default rule).</p>
<h4 id="rule-induction-using-a-sequential-covering-algorithm">Rule Induction Using a Sequential Covering Algorithm</h4>
<p>Greedy depth-first strategy to scan data to learn rules for each class.</p>
<h4 id="rule-quality-measures">Rule Quality Measures</h4>
<p>Accuracy alone is not a good measure. A rule can sacrifice coverage for a higher accuracy.</p>
<p>Entropy, or information gain measure, is better.</p>
<p>Another measure based on information gain was proposed in <strong>FOIL</strong> (First Order Inductive Leaner).</p>
<p>We can also use a statistic test of significance to determine if the apparent effect of a rule is not attributed to chance but instead indicates a genuine correlation between attribute values and classes. The <strong>likelihood ratio statistic</strong> can be found in <span class="citation" data-cites="Han:2011wk">[1]</span>.</p>
<h4 id="pruning-the-rule-set">Pruning the Rule Set</h4>
<p>Prune any condition that does not improve estimated accuracy of the rule for a given rule antecedent.</p>
<p>Prune rules that do not improve estimated accuracy of the rule set.</p>
<p>After pruning, the rule set may no longer be mutually exclusive and exhaustive. C4.5 adopts a <strong>class-based ordering scheme</strong>.</p>
<p>FOIL uses <span class="math">\[FOIL\_Prune(R) = \frac{pos - neg}{pos + neg}\]</span></p>
<h3 id="classifier-evaluation">Classifier Evaluation</h3>
<p>Terminology</p>
<ul>
<li><strong>positive tuples</strong> (P): Tuples of the main class of interest. Let P be the number of positive tuples.</li>
<li><strong>negative tuples</strong>: All other tuples. Let N be the number of negative tuples.</li>
<li><strong>True positives</strong> (TP): Positive tuples that were correctly labeled by the classifier. Let TP be the number of true positives.</li>
<li><strong>True negatives</strong> (TN): Negative tuples that were correctly labeled by the classifier. Let TN be the number of true negatives.</li>
<li><strong>False positives</strong> (FP): Negative tuples that were incorrectly labeled as positive. Let FP be the number of false positives.</li>
<li><strong>False negatives</strong> (FN): Positive tuples that were mislabeled as negative. Let FN be the number of false negatives.</li>
</ul>
<p>We have <span class="math">\(P = TP + FN, N = FP + TN\)</span>.</p>
<p>Evaluation measures</p>
<ul>
<li>accuracy <span class="math">\[accuracy = \frac{TP + TN}{P + N}\]</span></li>
<li>error rate, <span class="math">\(1 - accuracy\)</span> <span class="math">\[error\ rate = \frac{FP + FN}{P + N}\]</span>
<ul>
<li>If we use the training set to estimate the error rate of a model, this quantity is known as <strong>resubstitution error</strong></li>
</ul></li>
<li>sensitivity, or true positive rate, recall <span class="math">\[sensitivity = \frac{TP}{P}\]</span></li>
<li>specificity, or true negative rate <span class="math">\[specificity = \frac{TN}{N}\]</span>
<ul>
<li>Sensitivity and specificity can be used to solve class imbalance problem.</li>
</ul></li>
<li>precision <span class="math">\[precision = \frac{TP}{TP+FP}\]</span>
<ul>
<li>Precision can be thought of a measure of <em>exactness</em> (i.e., what percentage of tuples labeled as positive are actually such), whereas recall is a measure of <em>completeness</em> (what percentage of positive tuples are labeled as such.)</li>
</ul></li>
<li><span class="math">\(F_{1}\)</span> is harmonic mean of precision and recall <span class="math">\[F_{1} = \frac{2 \times precision \times recall}{precision +
  recall}\]</span></li>
<li><span class="math">\(F_{\beta}\)</span>, where <span class="math">\(\beta\)</span> is a non-negative real number <span class="math">\[F_{\beta} = \frac{(1 + {\beta}^{2}) \times precision \times
  recall}{{\beta}^{2} \times precision + recall}\]</span>
<ul>
<li>Combine precision and recall into a single measure. Commonly used <span class="math">\(F_{\beta}\)</span> measures are <span class="math">\(F_{2}\)</span> and <span class="math">\(F_{0.5}\)</span>.</li>
</ul></li>
</ul>
<p>If tuples can belong to multiple classes, the accuracy measure is not appropriate. It is useful to return a probability class distribution.</p>
<p>In addition to accuracy-based measures, classifiers can also be compared with respect to the following additional aspects:</p>
<ul>
<li><strong>Speed</strong>: computational cost to generate or use the classifier</li>
<li><strong>Robustness</strong>: ability to make correct predictions given noisy data or missing values</li>
<li><strong>Scalability</strong>: ability to construct the classifier efficiently given large amounts of data.</li>
<li><strong>Interpretability</strong>: level of understanding and insight that is provided by the classifier or predictor. This is subjective and therefore difficult to assess.</li>
</ul>
<h4 id="cross-validation">Cross-Validation</h4>
<p>In general, stratified 10-fold cross-validation is recommended for estimating accu- racy (even if computation power allows using more folds) due to its relatively low bias and variance.</p>
<h4 id="bootstrap">Bootstrap</h4>
<p>A commonly used one is the <strong>.632 bootstrap</strong>, which works as follows. Suppose we are given a data set of <span class="math">\(d\)</span> tuples. The data set is sampled d times, with replacement, resulting in a <em>bootstrap sample</em> or training set of <span class="math">\(d\)</span> samples. It is very likely that some of the original data tuples will occur more than once in this sample. The data tuples that did not make it into the training set end up forming the test set. Suppose we were to try this out several times. As it turns out, on average, 63.2% of the original data tuples will end up in the bootstrap sample, and the remaining 36.8% will form the test set (hence, the name, .632 bootstrap).</p>
<p>We can repeat the sampling procedure k times, where in each iteration, we use the current test set to obtain an accuracy estimate of the model obtained from the current bootstrap sample. The overall accuracy of the model, M, is then estimated as</p>
<p><span class="math">\[Acc(M) = \frac{1}{k}(0.632 \times Acc(M_{i})_{test\_set} + 0.368
\times Acc(M_{i})_{train\_set})\]</span></p>
<p>Bootstrapping tends to be overly optimistic. It works best with small data sets.</p>
<h4 id="statistical-tests-of-significance">Statistical Tests of Significance</h4>
<p>Mean error rates are just <em>estimates</em> of error on the true population of future data cases. Although the mean error rates obtained for models may appear different, that difference may not be statistically significant. What if any difference between the models may just be attributed to chance?</p>
<p>Suppose we have trained and tested two models, <span class="math">\(M_1\)</span> and <span class="math">\(M_2\)</span>. In such cases, we do a <strong>pairwise comparison</strong> of the two models for <em>each</em> 10-fold cross-validation round. That is, for the <span class="math">\(i\)</span>th round of 10-fold cross-validation, the same cross-validation partitioning is used to obtain an error rate for <span class="math">\(M_1\)</span> and for <span class="math">\(M_2\)</span>. Let <span class="math">\(err(M_1)_i\)</span> (or <span class="math">\(err(M_2)_i\)</span>) be the error rate of model <span class="math">\(M_1\)</span> (or <span class="math">\(M_2\)</span>) on round <span class="math">\(i\)</span>. The error rates for <span class="math">\(M_1\)</span> are averaged to obtain a mean error rate for <span class="math">\(M_1\)</span>, denoted <span class="math">\(\overline{err}(M_1)\)</span>. Similarly, we can obtain <span class="math">\(\overline{err}(M_2)\)</span>. The variance of the difference between the two models is denoted <span class="math">\(var(M_1 - M_2)\)</span>. The <em>t</em>-test computes the <em>t-statistic with k-1 degrees of freedom</em> for <em>k</em> samples. In our example, we have <em>k=10</em> (the <em>k</em> samples are error rates obtained from ten 10-fold cross-validations for each model.) The <em>t</em>-statistic for pairwise comparison is computed as follows: <span class="math">\[t = \frac{\overline{err}(M_1)-\overline{err}(M_2)}{\sqrt{var(M_1 - M_2)/k}},\]</span> where <span class="math">\[var(M_1 - M_2) = \frac{1}{k}
\sum_{i=1}^{k}{[err(M_1)_i - err(M_2)_i - (\overline{err}(M_1) - \overline{err}(M_2))]^2}.\]</span></p>
<p>Compute <span class="math">\(t\)</span> and select a <em>significance level</em>, usually 5% or 1% in practice. Then consult a table for the <em>t-distribution</em>. We shall tell whether the difference between <span class="math">\(M_1\)</span> and <span class="math">\(M_2\)</span> is statistically significant.</p>
<p>If two test sets are used instead of a common one, we shall use the nonpaired version of the <em>t</em>-test. The variance is estimated as <span class="math">\[var(M_1 - M_2) = \sqrt{\frac{var(M_1)}{k_1} + \frac{var(M_2}{k_2}},\]</span> and <span class="math">\(k_1\)</span> and <span class="math">\(k_2\)</span> are the number of cross-validation samples used for <span class="math">\(M_1\)</span> and <span class="math">\(M_2\)</span>, respectively. When consulting the table of <em>t</em>-distribution, the number of freedom used is taken as the minimum number of degrees of the two models.</p>
<h4 id="cost-benefit-and-roc-curves">Cost-Benefit and ROC Curves</h4>
<p>Up to now, we have assumed equal costs of true positives, false positives and false negatives. In real applications, such as medical prediction, they may have different costs (e.g. financial or timing).</p>
<p>Or we can assign costs to each decision. And the goal of the model could be to minimize the estimated cost of future decisions.</p>
<p><strong>Receiver operating characteristic</strong> curves show the trade-off between the <em>true positive rate (TPR)</em> and the <em>false positive rate (FPR)</em>. <span class="math">\[TPR = \frac{TP}{P}, FPR = \frac{FP}{N}\]</span> TPR is sensitivity mentioned in <em><a href="#classifier-evaluation">Classifier Evaluation section</a> and FPR is1-specificity</em>.</p>
<p>To plot an ROC curve for a given classification model, M, the model must be able to return a probability of the predicted class for each test tuple.</p>
<figure>
<img src="http://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png" alt="ROC Curve" /><figcaption>ROC Curve</figcaption>
</figure>
<p>The closer the area under ROC curve is to 1.0, the more accurate the model is.</p>
<h3 id="improving-classification-accuracy">Improving Classification Accuracy</h3>
<p>Bagging, boosting, and random forests are examples of <strong>ensemble methods</strong>, which combine a series of base classifiers to create an improved composite classification model.</p>
<ul>
<li><strong>Bagging</strong>
<ul>
<li>The term <em>bagging</em> stands for <em>bootstrap aggregation</em>. Each training set is a bootstrap sample.</li>
<li>The bagged classifier return by majority voting (or average value in continuous value prediction).</li>
<li>The bagged classifier often has significantly greater accuracy than a single classifier derived from <em>D</em>, the original training data. It will not be considerably worse and is more robust to the effects of noisy data and overfitting. The increased accuracy occurs because the composite model reduces the variance of the individual classifiers.</li>
</ul></li>
<li>Boosting</li>
<li>Random Forest</li>
</ul>
<h2 id="cluster-analysis">Cluster Analysis</h2>
<p>In many cases, class-labeled data may simply not exist at the beginning. <strong>Clustering</strong> can be used to generate class labels for a group of data.</p>
<p>The objects are clustered or grouped based on the principle of <em>maximizing the intraclass similarity and minimizing the interclass similarity</em>.</p>
<h2 id="outlier-analysis">Outlier Analysis</h2>
<p>A data set may contain objects that do not comply with the general behavior or model of the data. These data objects are <strong>outliers</strong>. Many data mining methods discard outliers as noise or exceptions. However, in some applications (e.g., fraud detection) the rare events can be more interesting than the more regularly occurring ones. The analysis of outlier data is referred to as <strong>outlier analysis</strong> or <strong>anomaly mining</strong>.</p>
<h1 id="which-patterns-are-interesting">Which Patterns Are Interesting?</h1>
<p>A pattern is interesting if it is</p>
<ol type="1">
<li><em>easily understood</em> by humans</li>
<li><em>valid</em> on new or test data with some degree of <em>certainty</em></li>
<li>potentially <em>useful</em></li>
<li><em>novel</em></li>
</ol>
<p>or if it validates a hypothesis that the user <em>sought to confirm</em>.</p>
<p>An interesting pattern represents <strong>knowledge</strong>.</p>
<h2 id="associationcorrelation-rules">Association/Correlation Rules</h2>
<p>If <span class="math">\(A ⇒ B\)</span> has a confidence of 70% while <span class="math">\(B\)</span> having a support of 85%, the rule might be misleading. Existence of <span class="math">\(A\)</span> actually decreases the occurrence of <span class="math">\(B\)</span>.</p>
<ul>
<li><strong>Lift</strong> assesses the degree to which the occurrence of <span class="math">\(A\)</span> “lifts” the occurrence of <span class="math">\(B\)</span>. [AY99] <span class="math">\[lift(A,B) = \frac{P(A∪B)}{P(A)P(B)}\]</span></li>
</ul>
<p><em>ChiSquare</em> and <em>lift</em> are sensitive to transactions that do not contain the itemsets of interest (<strong>null-transaction</strong>); they would generate unstable results. Therefore we need other measures which are <strong>null-invariant</strong>. Here are four of them.</p>
<ul>
<li><strong>All confidence</strong> <span class="math">\[all\_conf(A,B)
    = \frac{sup(A∪B)}{max(sup(A), sup(B))}
    = min(P(A|B),P(B|A))\]</span> <span class="math">\(all\_conf(A,B)\)</span> is the minimum confidence of the two association rules related to <span class="math">\(A\)</span> and <span class="math">\(B\)</span>, namely, <span class="math">\(A ⇒ B\)</span> and <span class="math">\(B ⇒ A\)</span>. [Omi03; LKCH03]</li>
<li><strong>Max confidence</strong> <span class="math">\[max\_conf(A,B) = max(P(A|B),P(B|A))\]</span> <span class="math">\(max\_conf(A,B)\)</span> measure is the maximum confidence of the two association rules, <span class="math">\(A ⇒ B\)</span> and <span class="math">\(B ⇒ A\)</span>.</li>
<li><strong>Kulczynski</strong> <span class="math">\[Kulc(A,B) = \frac{1}{2} (P(A|B) + P(B|A))\]</span> It is the average of two conditional probabilities: the probability of itemset <span class="math">\(B\)</span> given itemset <span class="math">\(A\)</span>, and the probability of itemset <span class="math">\(A\)</span> given itemset <span class="math">\(B\)</span>. [WCH10]</li>
<li><strong>Cosine</strong> <span class="math">\[cosine(A,B)
    = \frac{P(A∪B)}{\sqrt{P(A) \times P(B)}}
    = \frac{sup(A∪B)}{\sqrt{sup(A) \times sup(B)}}
    = \sqrt{P(A|B) \times P(B|A})\]</span> The cosine measure can be viewed as a <em>harmonized lift</em> measure.</li>
</ul>
<p>To compare the four null-invariant measures, we need <strong>imbalance ratio (IR)</strong>:</p>
<p><span class="math">\[IR(A,B) = \frac{|sup(A) − sup(B)|}{sup(A) + sup(B) − sup(A∪B)}\]</span></p>
<p>The two measures, <em>Kulc</em> and <em>IR</em>, work together, presenting a clear picture.</p>
<h1 id="data-preprocessing">Data Preprocessing</h1>
<p>Poor data quality may lead to poor analysis output. Some methods are susceptible to input data accuracy. Some won't take incomplete data. Other's will be affected by inconsistent data.</p>
<h2 id="data-cleaning">Data Cleaning</h2>
<p>Missing values</p>
<ul>
<li>Ignore</li>
<li>Fill in manually</li>
<li>Automatically fill in constants (e.g. NULL, Unknown)
<ul>
<li>Null values can be different. How to tell &quot;I don't remember that.&quot; from &quot;I do not have one.&quot;?</li>
</ul></li>
<li>Fill in mean/median/estimation</li>
</ul>
<p>Noisy data</p>
<ul>
<li>Binning: replace values by bin mean/median/boundaries</li>
<li>Regression: replace values with regression results</li>
<li>Outlier analysis: remove outliers</li>
</ul>
<h2 id="data-integration">Data Integration</h2>
<p>Entity Identification Problem</p>
<ul>
<li>ID</li>
<li>Manual mapping</li>
</ul>
<p>Redundancy from multiple sources</p>
<ul>
<li>Correlation analysis</li>
</ul>
<p>Tuple duplication</p>
<ul>
<li>Two different IDs assigned to the same person</li>
</ul>
<p>Data value conflict</p>
<ul>
<li>different <em>unit</em></li>
<li>different <em>tax rates</em></li>
<li>GPA [0,4] vs [0,5] vs [0,100]</li>
</ul>
<h2 id="data-reduction">Data Reduction</h2>
<p><strong>Data reduction</strong> techniques can be applied to obtain a reduced representation of the data set that is much smaller in volume, yet closely maintains the integrity of the original data.</p>
<ul>
<li>Dimension reduction
<ul>
<li>Wavelet transform
<ul>
<li>fast DWT, <em>O(n)</em></li>
</ul></li>
<li>PCA<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup></li>
<li>Attribute(feature) subset selection</li>
</ul></li>
<li>Numerosity reduction</li>
<li>Parametric method: use a model to estimate the data; store only the model (and outliers if necessary)
<ul>
<li>regression models</li>
</ul></li>
<li>Nonparametric method
<ul>
<li>Histogram</li>
<li>Clustering</li>
<li>Sampling</li>
<li>Data cube aggregation</li>
</ul></li>
<li>Data compression</li>
<li>lossless</li>
<li>lossy</li>
</ul>
<h2 id="data-transformation-and-data-discretization">Data Transformation and Data Discretization</h2>
<p>Strategies for data transformation include the following:</p>
<ul>
<li><strong>Smoothing</strong>, which works to remove noise from the data.</li>
<li><strong>Attribute construction</strong> (or feature construction), where new attributes are constructed and added from the given set of attributes to help the mining process.</li>
<li><strong>Aggregation</strong>, where summary or aggregation operations are applied to the data</li>
<li><strong>Normalization</strong>, where the attribute data are scaled so as to fall within a smaller range, such as −1.0 to 1.0, or 0.0 to 1.0.</li>
<li><strong>Discretization</strong>, where the raw values of a numeric attribute (e.g., age) are replaced by interval labels (e.g., 0–10, 11–20, etc.) or conceptual labels (e.g., youth, adult, senior).</li>
<li><strong>Concept hierarchy generation for nominal data</strong>, where attributes such as street can be generalized to higher-level concepts, like city or country.</li>
</ul>
<h3 id="normalization">Normalization</h3>
<ul>
<li>Min-max normalization</li>
<li>z-score normalization</li>
<li>Decimal scaling</li>
</ul>
<h3 id="discretization">Discretization</h3>
<ul>
<li>Binning</li>
<li>Histogram Analysis</li>
<li>Cluster, Decision Tree, Correlation Analysis</li>
<li>ChiMerge</li>
</ul>
<h1 id="references">References</h1>
<p>[1]J. Han, M. Kamber, and J. Pei, <em>Data Mining: Concepts and Techniques</em>, 3rd ed. Morgan Kaufmann, 2011.</p>
<p>[2]R. Agrawal and R. Srikant, “Fast algorithms for mining association rules,” <em>Proc 20th Int Conf Very Large Data Bases</em>, 1994.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Sometimes data transformation and consolidation are performed before the data selection process, particularly in the case of data warehousing. <em>Data reduction</em> may also be performed to obtain a smaller representation of the original data without sacrificing its integrity.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>PCA tends to be better at handling sparse data, whereas wavelet transforms are more suitable for data of high dimensionality.<a href="#fnref2">↩</a></p></li>
</ol>
</section>
</body>
</html>
