<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="stylesheets/styles.css">
</head>
<body>
<h1 id="notes-on-knowledge-discovery-from-data-kdd">Notes on Knowledge Discovery from Data (KDD)</h1>
<p><a id="whatiskdd"></a> # What is KDD?</p>
<ul>
<li>KDD is a process that attempts to discover <strong>patterns</strong> in large data sets</li>
<li>KDD's overall goal is to extract information from a data set and transform it into an <strong>understandable structure</strong> for further use, e.g. a decision support system</li>
</ul>
<p>KDD process can be shown as an iterative sequence of the following steps [1]:</p>
<ol type="1">
<li><a href="#datacleaning"><strong>Data cleaning</strong></a> (to remove noise and inconsistent data)</li>
<li><a href="#dataintegration"><strong>Data integration</strong></a> (where multiple data sources are combined)</li>
<li><strong>Data selection</strong> (where data relevant to the analysis task are retrieved from the database)</li>
<li><a href="#datatransformation"><strong>Data transformation</strong></a> (where data are transformed and consolidated into forms appropriate for mining by performing summary or aggregation operations) <sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup></li>
<li><a href="#patternkind"><strong>Data mining</strong></a> (an essential process where intelligent methods are applied to extract data patterns)</li>
<li><strong>Pattern evaluation</strong> (to identify the truly <a href="#interestingpattern">interesting patterns</a> representing knowledge based on <em>interestingness measures</em>)</li>
<li><strong>Knowledge presentation</strong> (where visualization and knowledge representation techniques are used to present mined knowledge to users)</li>
</ol>
<p><a id="patternkind"></a> # What Kinds of Patterns Can Be Mined?</p>
<h2 id="classconcept-description-characterization-and-discrimination">Class/Concept Description: Characterization and Discrimination</h2>
<p><strong>Data characterization</strong> is a summarization of the general characteristics or features of a target class of data.</p>
<ul>
<li>Statistical measures</li>
<li>OLAP</li>
<li>Attribute-oriented Induction</li>
</ul>
<p><strong>Data discrimination</strong> is a comparison of the general features of the target class data objects against the general features of objects from one or multiple contrasting classes.</p>
<h2 id="mining-frequent-patterns-associations-and-correlations">Mining Frequent Patterns, Associations, and Correlations</h2>
<p><strong>Frequent patterns</strong>: Itemsets, subsequences or sub structures that appear frequently in the data set.</p>
<p>Support &amp; Confidence</p>
<pre><code>support(A⇒B)=P(A∪B)
confidence (A⇒B) =P(B|A)</code></pre>
<p>Association rule</p>
<pre><code>computer ⇒ antivirus software [support = 2%, confidence = 60%]</code></pre>
<p><strong>Strong</strong> association rule: rules that satisfy both a minimum support threshold (<em>min_sup</em>) and a minimum confidence threshold (<em>min_conf</em>).</p>
<p>Association rule mining</p>
<ol type="1">
<li><p>Find all frequent item sets with at least <em>min_sup</em>.</p>
<ul>
<li>The number of frequent itemsets is too large for a large data set. If we have a frequent k-itemset, the total number of frequent itemsets that it contains is <code>2^k - 1</code>.</li>
<li>Finding closed &amp; maximal frequent itemsets
<ul>
<li><strong>closed frequent itemset</strong>: no frequent super-itemset with same support</li>
<li><strong>maximal frequent itemset</strong>: no frequent super-itemset</li>
<li>item merging</li>
<li>sub-itemset pruning</li>
<li>item skipping</li>
</ul></li>
</ul></li>
<li><p>Generate strong association rules from the frequent itemsets.</p></li>
</ol>
<p>Correlation rule</p>
<pre><code>A ⇒ B [support, confidence, correlation]</code></pre>
<h3 id="algorithms">Algorithms</h3>
<ul>
<li><strong>Apriori</strong> [2]: Finding frequent itemsets by confined candidate generation
<ol type="1">
<li>Hashing itemsets into buckets [PCY95a]</li>
<li>transaction reduction</li>
<li>partitioning the data to find candidate itemsets [SON95]</li>
<li>sampling: mining on a subset of the given data [Toi96]</li>
<li>dynamic itemset counting: start with count-so-far; fewer database scans [BMUT97]</li>
<li>parallel and distributed association mining [PCY95b][AS96] [CHN+96][ZPOL97]</li>
</ol></li>
<li><strong>A-Close</strong> [PBTL99]: Finding frequent closed itemsets</li>
<li><strong>FPgrowth</strong> [HPY00]: Pattern-growth approach for mining frequent itemsets</li>
<li><strong>CLOSET</strong> [PHM00]: Closed itemset mining based on FPgrowth</li>
<li><strong>Eclat</strong> (Equivalence Class Transformation) [Zak00]: mining frequent itemsets using the vertical data format</li>
</ul>
<h2 id="classification-and-regression-for-predictive-analysis">Classification and Regression for Predictive Analysis</h2>
<p><strong>Classification</strong> is the process of finding a <strong>model</strong> (or function) that describes and distinguishes data classes or concepts. The model are derived based on the analysis of a set of <strong>training data</strong> (i.e., data objects for which the class labels are known). The model is used to predict the class label of objects for which the the class label is unknown.</p>
<p>Model representation forms:</p>
<ul>
<li><strong>classification rules</strong> (i.e. IF-THEN rules)</li>
<li><strong>decision tree</strong></li>
<li><strong>neural network</strong></li>
</ul>
<p>Whereas classification predicts <em>categorical</em> (discrete, unordered) labels, <strong>regression</strong> models <em>continuous-valued</em> functions.</p>
<p><strong>Regression analysis</strong> is a statistical methodology that is most often used for numeric prediction, although other methods exist as well. Regression also encompasses the identification of distribution <em>trends</em> based on the available data.</p>
<h2 id="cluster-analysis">Cluster Analysis</h2>
<p>In many cases, class-labeled data may simply not exist at the beginning. <strong>Clustering</strong> can be used to generate class labels for a group of data.</p>
<p>The objects are clustered or grouped based on the principle of <em>maximizing the intraclass similarity and minimizing the interclass similarity</em>.</p>
<h2 id="outlier-analysis">Outlier Analysis</h2>
<p>A data set may contain objects that do not comply with the general behavior or model of the data. These data objects are <strong>outliers</strong>. Many data mining methods discard outliers as noise or exceptions. However, in some applications (e.g., fraud detection) the rare events can be more interesting than the more regularly occurring ones. The analysis of outlier data is referred to as <strong>outlier analysis</strong> or <strong>anomaly mining</strong>.</p>
<p><a id="interestingpattern"></a> # Which Patterns Are Interesting?</p>
<p>A pattern is interesting if it is</p>
<ol type="1">
<li><em>easily understood</em> by humans</li>
<li><em>valid</em> on new or test data with some degree of <em>certainty</em></li>
<li>potentially <em>useful</em></li>
<li><em>novel</em></li>
</ol>
<p>or if it validates a hypothesis that the user <em>sought to confirm</em>.</p>
<p>An interesting pattern represents <strong>knowledge</strong>.</p>
<h2 id="associationcorrelation-rules">Association/Correlation Rules</h2>
<p>If <code>A ⇒ B</code> has a confidence of 70% while <code>B</code> having a support of 85%, the rule might be misleading. Existence of <code>A</code> actually decreases the occurance of <code>B</code>.</p>
<h3 id="lift">Lift</h3>
<p><strong>Lift</strong> assesses the degree to which the occurrence of <code>A</code> “lifts” the occurrence of <code>B</code>. [AY99]</p>
<pre><code>lift(A,B) = P(A∪B) / ( P(A)P(B) )</code></pre>
<h3 id="chisquare">ChiSquare</h3>
<p><em>ChiSquare</em> and <em>lift</em> are sensitive to transactions that do not contain the itemsets of interest (<strong>null-transaction</strong>); they would generate unstable results.</p>
<p>Therefore we need other measures which are <strong>null-invariant</strong>. Here are four of them.</p>
<h3 id="all-confidence">All confidence</h3>
<pre><code>all_conf(A,B) = sup(A∪B) / ( max {sup(A), sup(B)} )
              = min{P(A|B),P(B|A)}</code></pre>
<p><code>all_conf(A,B)</code> is the minimum confidence of the two association rules related to <code>A</code> and <code>B</code>, namely, <code>A ⇒ B</code> and <code>B ⇒ A</code>. [Omi03][LKCH03]</p>
<h3 id="max-confidence">Max confidence</h3>
<pre><code>max_conf(A,B) = max{P(A|B),P(B|A)}</code></pre>
<p><code>max_conf</code> measure is the maximum confidence of the two association rules, <code>A ⇒ B</code> and <code>B ⇒ A</code>.</p>
<h3 id="kulczynski">Kulczynski</h3>
<pre><code>Kulc(A,B) = 1/2 (P(A|B) + P(B|A))</code></pre>
<p>It is the average of two conditional probabilities: the probability of itemset <code>B</code> given itemset <code>A</code>, and the probability of itemset <code>A</code> given itemset <code>B</code>. [WCH10]</p>
<h3 id="cosine">Cosine</h3>
<pre><code>cosine(A,B) = P(A∪B) / sqrt(P(A) × P(B))
            = sup(A∪B) / sqrt(sup(A) × sup(B))
            = sqrt(P(A|B) × P(B|A))</code></pre>
<p>The cosine measure can be viewed as a <em>harmonized lift</em> measure.</p>
<p>To compare the four null-invariant measures, we need <strong>imbalance ratio (IR)</strong>:</p>
<pre><code>IR(A,B) = |sup(A) − sup(B)| / (sup(A) + sup(B) − sup(A∪B))</code></pre>
<p>The two measures, <em>Kulc</em> and <em>IR</em>, work together, presenting a clear picture.</p>
<h2 id="measures-of-pattern-interestingness">Measures of Pattern Interestingness</h2>
<ul>
<li>For classification rules
<ul>
<li>accuracy</li>
<li>coverage</li>
</ul></li>
</ul>
<p><a id="datapreprocessing"></a> # Data Preprocessing</p>
<p>Poor data quality may lead to poor analysis output. Some methods are susceptible to input data accuracy. Some won't take incomplete data. Other's will be affected by inconsistent data.</p>
<p><a id="datacleaning"></a> ## Data Cleaning</p>
<p>Missing values</p>
<ul>
<li>Ignore</li>
<li>Fill in manually</li>
<li>Automatically fill in constants (e.g. NULL, Unknown)
<ul>
<li>Null values can be different. How to tell &quot;I don't remember that.&quot; from &quot;I do not have one.&quot;?</li>
</ul></li>
<li>Fill in mean/median/estimation</li>
</ul>
<p>Noisy data</p>
<ul>
<li>Binning: replace values by bin mean/median/boundaries</li>
<li>Regression: replace values with regression results</li>
<li>Outlier analysis: remove outliers</li>
</ul>
<p><a id="dataintegration"></a> ## Data Integration</p>
<p>Entity Identification Problem</p>
<ul>
<li>ID</li>
<li>Manual mapping</li>
</ul>
<p>Redundancy from multiple sources</p>
<ul>
<li>Correlation analysis</li>
</ul>
<p>Tuple duplication</p>
<ul>
<li>Two different IDs assigned to the same person</li>
</ul>
<p>Data value conflict</p>
<ul>
<li>different <em>unit</em></li>
<li>different <em>tax rates</em></li>
<li>GPA [0,4] vs [0,5] vs [0,100]</li>
</ul>
<h2 id="data-reduction">Data Reduction</h2>
<p><strong>Data reduction</strong> techniques can be applied to obtain a reduced representation of the data set that is much smaller in volume, yet closely maintains the integrity of the original data.</p>
<ul>
<li>Dimension reduction
<ul>
<li>Wavelet transform
<ul>
<li>fast DWT, <em>O(n)</em></li>
</ul></li>
<li>PCA<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup></li>
<li>Attribute(feature) subset selection</li>
</ul></li>
<li>Numerosity reduction
<ul>
<li>Parametric method: use a model to estimate the data; store only the model (and outliers if necessary)
<ul>
<li>regression models</li>
</ul></li>
<li>Nonparametric method
<ul>
<li>Histogram</li>
<li>Clustering</li>
<li>Sampling</li>
<li>Data cube aggregation</li>
</ul></li>
</ul></li>
<li>Data compression
<ul>
<li>lossless</li>
<li>lossy</li>
</ul></li>
</ul>
<p><a id="datatransformation"></a> ## Data Transformation and Data Discretization</p>
<p>Strategies for data transformation include the following:</p>
<ol type="1">
<li><strong>Smoothing</strong>, which works to remove noise from the data.2. <strong>Attribute construction</strong> (or feature construction), where new attributes are constructed and added from the given set of attributes to help the mining process.3. <strong>Aggregation</strong>, where summary or aggregation operations are applied to the data4. <strong>Normalization</strong>, where the attribute data are scaled so as to fall within a smaller range, such as −1.0 to 1.0, or 0.0 to 1.0.5. <strong>Discretization</strong>, where the raw values of a numeric attribute (e.g., age) are replaced by interval labels (e.g., 0–10, 11–20, etc.) or conceptual labels (e.g., youth, adult, senior).</li>
<li><strong>Concept hierarchy generation for nominal data</strong>, where attributes such as street can be generalized to higher-level concepts, like city or country.</li>
</ol>
<h3 id="normalization">Normalization</h3>
<ul>
<li>Min-max normalization</li>
<li>z-score normalization</li>
<li>Decimal scaling</li>
</ul>
<h3 id="discretization">Discretization</h3>
<ul>
<li>Binning</li>
<li>Histogram Analysis</li>
<li>Cluster, Decision Tree, Correlation Analysis
<ul>
<li>ChiMerge</li>
</ul></li>
</ul>
<p><a id="references"></a> # References</p>
<p>[1] Han, Jiawei, Micheline Kamber, and Jian Pei. Data Mining: Concepts and Techniques. 3rd ed. Morgan Kaufmann, 2011.</p>
<p>[2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In Proc. 1994 Int. Conf. Very Large Data Bases (VLDB’94), pp. 487–499, Santiago, Chile, Sept. 1994.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Sometimes data transformation and consolidation are performed before the data selection process, particularly in the case of data warehousing. <em>Data reduction</em> may also be performed to obtain a smaller representation of the original data without sacrificing its integrity.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>PCA tends to be better at handling sparse data, whereas wavelet transforms are more suitable for data of high dimensionality.<a href="#fnref2">↩</a></p></li>
</ol>
</section>
</body>
</html>
