<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="stylesheets/styles.css">
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="notes-on-knowledge-discovery-from-data-kdd">Notes on Knowledge Discovery from Data (KDD)</h1>
<p>by Pei QING <script type="text/javascript">
<!--
h='&#x63;&#x6f;&#x6d;&#112;&#46;&#112;&#x6f;&#108;&#x79;&#x75;&#46;&#x65;&#100;&#x75;&#46;&#104;&#x6b;';a='&#64;';n='&#x63;&#x73;&#112;&#x71;&#x69;&#110;&#x67;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>&#x63;&#x73;&#112;&#x71;&#x69;&#110;&#x67;&#32;&#x61;&#116;&#32;&#x63;&#x6f;&#x6d;&#112;&#32;&#100;&#x6f;&#116;&#32;&#112;&#x6f;&#108;&#x79;&#x75;&#32;&#100;&#x6f;&#116;&#32;&#x65;&#100;&#x75;&#32;&#100;&#x6f;&#116;&#32;&#104;&#x6b;</noscript></p>
<h1 id="what-is-kdd">What Is KDD?</h1>
<ul>
<li>KDD is a process that attempts to discover <strong>patterns</strong> in large data sets</li>
<li>KDD’s overall goal is to extract information from a data set and transform it into an <strong>understandable structure</strong> for further use, e.g. a decision support system.</li>
</ul>
<p>KDD process can be shown as an iterative sequence of the following steps <span class="citation" data-cites="Han:2011wk">[1]</span>:</p>
<ol type="1">
<li><a href="#data-cleaning"><strong>Data cleaning</strong></a> (to remove noise and inconsistent data)</li>
<li><a href="#data-integration"><strong>Data integration</strong></a> (where multiple data sources are combined)</li>
<li><strong>Data selection</strong> (where data relevant to the analysis task are retrieved from the database) 4. <a href="#data-transformation-and-data-discretization"><strong>Data transformation</strong></a> (where data are transformed and consolidated into forms appropriate for mining by performing summary or aggregation operations)<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup></li>
<li><strong>Data mining</strong> (an essential process where intelligent methods are applied to extract data patterns)</li>
<li><strong>Pattern evaluation</strong> (to identify the truly <a href="#which-patterns-are-interesting">interesting patterns</a> representing knowledge based on <em>interestingness measures</em>)</li>
<li><strong>Knowledge presentation</strong> (where visualization and knowledge representation techniques are used to present mined knowledge to users)</li>
</ol>
<h1 id="what-kinds-of-patterns-can-be-mined">What Kinds of Patterns Can Be Mined?</h1>
<h2 id="classconcept-description-characterization-and-discrimination">Class/Concept Description: Characterization and Discrimination</h2>
<p><strong>Data characterization</strong> is a summarization of the general characteristics or features of a target class of data.</p>
<ul>
<li>Statistical measures</li>
<li>OLAP</li>
<li>Attribute-oriented Induction</li>
</ul>
<p><strong>Data discrimination</strong> is a comparison of the general features of the target class data objects against the general features of objects from one or multiple contrasting classes.</p>
<h2 id="mining-frequent-patterns-associations-and-correlations">Mining Frequent Patterns, Associations, and Correlations</h2>
<p><strong>Frequent patterns</strong>: Itemsets, subsequences or sub structures that appear frequently in the data set.</p>
<p>Support &amp; Confidence</p>
<p><span class="math">\[support(A⇒B) = P(A∪B)\]</span></p>
<p><span class="math">\[confidence(A⇒B) = P(B|A)\]</span></p>
<p>Association rule</p>
<p><span class="math">\[computer ⇒ antivirus\ software [support = 2\%, confidence = 60\%]\]</span></p>
<p><strong>Strong</strong> association rule: rules that satisfy both a minimum support threshold (<em>min_sup</em>) and a minimum confidence threshold (<em>min_conf</em>).</p>
<p>Association rule mining</p>
<ol type="1">
<li><p>Find all frequent item sets with at least <em>min_sup</em>.</p>
<ul>
<li>The number of frequent itemsets is too large for a large data set. If we have a frequent k-itemset, the total number of frequent itemsets that it contains is <code>2^k - 1</code>.</li>
<li>Finding closed &amp; maximal frequent itemsets
<ul>
<li><strong>closed frequent itemset</strong>: no frequent super-itemset with same support</li>
<li><strong>maximal frequent itemset</strong>: no frequent super-itemset</li>
<li>item merging</li>
<li>sub-itemset pruning</li>
<li>item skipping</li>
</ul></li>
</ul></li>
<li><p>Generate strong association rules from the frequent itemsets.</p></li>
</ol>
<p>Correlation rule</p>
<p><span class="math">\[A ⇒ B [support, confidence, correlation]\]</span></p>
<h3 id="algorithms">Algorithms</h3>
<ul>
<li><strong>Apriori</strong> <span class="citation" data-cites="Agrawal:1994ut">[2]</span>: Finding frequent itemsets by confined candidate generation
<ol type="1">
<li>Hashing itemsets into buckets [PCY95a]</li>
<li>transaction reduction</li>
<li>partitioning the data to find candidate itemsets [SON95]</li>
<li>sampling: mining on a subset of the given data [Toi96]</li>
<li>dynamic itemset counting: start with count-so-far; fewer database scans [BMUT97]</li>
<li>parallel and distributed association mining [PCY95b][AS96] [CHN+96][ZPOL97]</li>
</ol></li>
<li><strong>A-Close</strong> [PBTL99]: Finding frequent closed itemsets</li>
<li><strong>FPgrowth</strong> [HPY00]: Pattern-growth approach for mining frequent itemsets</li>
<li><strong>CLOSET</strong> [PHM00]: Closed itemset mining based on FPgrowth</li>
<li><strong>Eclat</strong> (Equivalence Class Transformation) [Zak00]: mining frequent itemsets using the vertical data format</li>
</ul>
<h2 id="classification-and-regression-for-predictive-analysis">Classification and Regression for Predictive Analysis</h2>
<p><strong>Classification</strong> is the process of finding a <strong>model</strong> (or function) that describes and distinguishes data classes or concepts. The model are derived based on the analysis of a set of <strong>training data</strong> (i.e., data objects for which the class labels are known). The model is used to predict the class label of objects for which the the class label is unknown.</p>
<p>Model representation forms:</p>
<ul>
<li><strong>classification rules</strong> (i.e. IF-THEN rules)</li>
<li><a href="#decision-tree"><strong>decision tree</strong></a></li>
<li><strong>neural network</strong></li>
</ul>
<p>Whereas classification predicts <em>categorical</em> (discrete, unordered) labels, <strong>regression</strong> models <em>continuous-valued</em> functions.</p>
<p><strong>Regression analysis</strong> is a statistical methodology that is most often used for numeric prediction, although other methods exist as well. Regression also encompasses the identification of distribution <em>trends</em> based on the available data.</p>
<h3 id="decision-tree">Decision Tree</h3>
<p>A <strong>decision tree</strong> is a flowchart-like tree structure, where each <strong>internal node</strong> (non- leaf node) denotes a test on an attribute, each <strong>branch</strong> represents an outcome of the test, and each <strong>leaf node</strong> (or terminal node) holds a class label.</p>
<h4 id="algorithms-to-generate-decision-trees">Algorithms to Generate Decision Trees</h4>
<ul>
<li><strong>ID3</strong></li>
<li><strong>C4.5</strong></li>
<li><strong>CART</strong></li>
<li>CHAID, popular in marketing</li>
</ul>
<p>At each level, select the attribute in the <strong>attribute list</strong> that, according to the <strong>attribute selection method</strong>, “best” discriminates the given tuples. If no attribute is available at current level, return the majority class.</p>
<p>ID3 uses <strong>information gain</strong> as its attribute selection measure. This is biased toward attributes with a large number of values. (E.g. N items have unique IDs. Selecting ID as the “best” attribute returns N nodes each with only one tuple.)</p>
<p><span class="math">\[Info(D) = -\sum_{i=1}^{m}{p_{i} log_2(p_{i})}\]</span> <span class="math">\[Info_{A}(D) = \sum_{j=1}^{v}{\frac{|D_j|}{|D|}\times Info(D_{j})}\]</span> <span class="math">\[Gain(A) = Info(D) - Info_{A}(D)\]</span></p>
<p>C4.5 uses <strong>gain ratio</strong> to overcome this bias by normalizing information gain.</p>
<p><span class="math">\[SplitInfo_{A}(D) = -\sum_{j=1}^{v}{\frac{|D_j|}{|D|} \times
log_{2}(\frac{|D_j|}{|D|})}\]</span> <span class="math">\[GainRatio(A) = \frac{Gain(A)}{SplitInfo_{A}(D)}\]</span></p>
<p>CART uses <strong>Gini index</strong>.</p>
<p><span class="math">\[Gini(D) = 1 - \sum_{i=1}^{m}{p_{i}^2}\]</span></p>
<p>CHAID uses a measure based on ChiSquare test.</p>
<p>Other attribute selection measures include</p>
<ul>
<li>C-SEP</li>
<li>G-statistic</li>
<li><strong>Minimum Description Length (MDL)</strong>: Encode both the <em>tree</em> and <em>exceptions</em> to the tree. Attribute that requires least bits for encoding is the best. It is least bias toward multivalued attributes.</li>
<li>multivariate splits: partition based on a <em>combination</em> of attributes. CART can find multivariate splits based on a linear combination of attributes. This is a form of <strong>attribute construction.</strong></li>
</ul>
<p>Measures that tend to produce <em>shallower trees</em> may be preferred. (Though shallower trees tend to have a large number of leaves and higher error rates.)</p>
<h4 id="tree-pruning">Tree Pruning</h4>
<p><strong>Tree pruning</strong> is a method to address the <em>overfitting</em> problem.</p>
<ul>
<li>The <strong>cost complexity</strong> pruning algorithm used in CART is an example of the postpruning approach. This approach considers the cost complexity of a tree to be a function of the number of leaves in the tree and the error rate of the tree (where the <strong>error rate</strong> is the percentage of tuples misclassified by the tree).</li>
<li>C4.5 uses <strong>pessimistic pruning</strong> in which it uses training set to estimate error rates.</li>
<li>MDL prunes a tree based on number of bits required to encode it.</li>
</ul>
<h4 id="problems-with-decision-trees">Problems with Decision Trees</h4>
<ul>
<li><strong>Repetition</strong>: The same attribute may be tested repeatedly along a branch. E.g. “price &gt; 50?”, then “price &lt; 100?”, followed by “100 &gt; price &gt; 80”, …</li>
<li><strong>Replication</strong>: Duplicate subtrees exist within the tree (, possibly in different branches.)</li>
</ul>
<p>Use multivariate splits to relieve these problems. Or use a rule-based classifier instead of a decision tree classifier.</p>
<ul>
<li>Scalability: ID3, C4.5 and CART works for small data sets. What if the data set is disk-resident?
<ul>
<li><strong>RainForest</strong>: maintains an AVC-set (attribute-value, classlabel)</li>
<li><strong>BOAT</strong> (Bootstrapped Optimistic Algorithm for Tree Construction): bootstrapping to create smaller samples of the training data that fits in memory. Each subset is used to construct a tree. The trees are examined and used to construct a new tree that is “very close” to the tree that would have been generated if all training data had fit in memory.
<ul>
<li>2~3x faster than RainForest while constructing exactly the same tree.</li>
<li>Incremental updates. BOAT can take new insertions and deletions for the training data without having to reconstruct the tree.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="bayes-classification">Bayes Classification</h3>
<p><code>P(H)</code>, <code>P(X|H)</code> and <code>P(X)</code> may be estimated from the given data. The posterior probability, <code>P(H|X)</code>, can be calculated from them. Bayes’s theorem:</p>
<p><span class="math">\[P(H|X) = \frac{P(X|H)P(H)}{P(X)}\]</span></p>
<p>Naive Bayesian Classifier</p>
<ul>
<li>assumption: <strong>class-conditional independence</strong></li>
<li>maximize <code>P(X|Ci)</code></li>
<li>Note: If we have a probability of zero, <strong>Laplacian correction</strong> can help.</li>
</ul>
<h3 id="rule-based-classification">Rule-Based Classification</h3>
<p>An IF-THEN rule:</p>
<pre><code>IF *condition* THEN *conclusion*.</code></pre>
<p>A rule <code>R</code> can be assessed by its <strong>coverage</strong> and <strong>accuracy</strong>.</p>
<p><span class="math">\[coverage(R) = \frac{n_{covers}}{|D|}\]</span></p>
<p><span class="math">\[accuracy(R) = \frac{n_{correct}}{n_{covers}}\]</span></p>
<h4 id="extracting-classification-rules-from-a-decision-tree">Extracting Classification Rules From a Decision Tree</h4>
<p>Each leaf forms a rule. The rules are <strong>mutually exclusive</strong> (no conflicted rules) and <strong>exhaustive</strong> (no need for a default rule).</p>
<h4 id="rule-induction-using-a-sequential-covering-algorithm">Rule Induction Using a Sequential Covering Algorithm</h4>
<p>Greedy depth-first strategy to scan data to learn rules for each class.</p>
<h4 id="rule-quality-measures">Rule Quality Measures</h4>
<p>Accuracy alone is not a good measure. A rule can sacrifice coverage for a higher accuracy.</p>
<p>Entropy, or information gain measure, is better.</p>
<p>Another measure based on information gain was proposed in <strong>FOIL</strong> (First Order Inductive Leaner).</p>
<p>We can also use a statistic test of significance to determine if the apparent effect of a rule is not attributed to chance but instead indicates a genuine correlation between attribute values and classes. The <strong>likelihood ratio statistic</strong> can be found in pp.363.</p>
<h4 id="pruning-the-rule-set">Pruning the Rule Set</h4>
<p>Prune any condition that does not improve estimated accuracy of the rule for a given rule antecedent.</p>
<p>Prune rules that do not improve estimated accuracy of the rule set.</p>
<p>After pruning, the rule set may no longer be mutually exclusive and exhaustive. C4.5 adopts a <strong>class-based ordering scheme</strong>.</p>
<p>FOIL uses <span class="math">\[FOIL_Prune(R) = \frac{pos - neg}{pos + neg}\]</span></p>
<h3 id="model-evaluation">Model Evaluation</h3>
<h2 id="cluster-analysis">Cluster Analysis</h2>
<p>In many cases, class-labeled data may simply not exist at the beginning. <strong>Clustering</strong> can be used to generate class labels for a group of data.</p>
<p>The objects are clustered or grouped based on the principle of <em>maximizing the intraclass similarity and minimizing the interclass similarity</em>.</p>
<h2 id="outlier-analysis">Outlier Analysis</h2>
<p>A data set may contain objects that do not comply with the general behavior or model of the data. These data objects are <strong>outliers</strong>. Many data mining methods discard outliers as noise or exceptions. However, in some applications (e.g., fraud detection) the rare events can be more interesting than the more regularly occurring ones. The analysis of outlier data is referred to as <strong>outlier analysis</strong> or <strong>anomaly mining</strong>.</p>
<h1 id="which-patterns-are-interesting">Which Patterns Are Interesting?</h1>
<p>A pattern is interesting if it is</p>
<ol type="1">
<li><em>easily understood</em> by humans</li>
<li><em>valid</em> on new or test data with some degree of <em>certainty</em></li>
<li>potentially <em>useful</em></li>
<li><em>novel</em></li>
</ol>
<p>or if it validates a hypothesis that the user <em>sought to confirm</em>.</p>
<p>An interesting pattern represents <strong>knowledge</strong>.</p>
<h2 id="associationcorrelation-rules">Association/Correlation Rules</h2>
<p>If <code>A ⇒ B</code> has a confidence of 70% while <code>B</code> having a support of 85%, the rule might be misleading. Existence of <code>A</code> actually decreases the occurrence of <code>B</code>.</p>
<h3 id="lift">Lift</h3>
<p><strong>Lift</strong> assesses the degree to which the occurrence of <code>A</code> “lifts” the occurrence of <code>B</code>. [AY99]</p>
<p><span class="math">\[lift(A,B) = \frac{P(A∪B)}{P(A)P(B)}\]</span></p>
<h3 id="chisquare">ChiSquare</h3>
<p><em>ChiSquare</em> and <em>lift</em> are sensitive to transactions that do not contain the itemsets of interest (<strong>null-transaction</strong>); they would generate unstable results.</p>
<p>Therefore we need other measures which are <strong>null-invariant</strong>. Here are four of them.</p>
<h3 id="all-confidence">All confidence</h3>
<p><span class="math">\[all\_conf(A,B)
    = \frac{sup(A∪B)}{max(sup(A), sup(B))}
    = min(P(A|B),P(B|A))\]</span></p>
<p><code>all_conf(A,B)</code> is the minimum confidence of the two association rules related to <code>A</code> and <code>B</code>, namely, <code>A ⇒ B</code> and <code>B ⇒ A</code>. [Omi03][LKCH03]</p>
<h3 id="max-confidence">Max confidence</h3>
<p><span class="math">\[max\_conf(A,B) = max(P(A|B),P(B|A))\]</span></p>
<p><code>max_conf</code> measure is the maximum confidence of the two association rules, <code>A ⇒ B</code> and <code>B ⇒ A</code>.</p>
<h3 id="kulczynski">Kulczynski</h3>
<p><span class="math">\[Kulc(A,B) = \frac{1}{2} (P(A|B) + P(B|A))\]</span></p>
<p>It is the average of two conditional probabilities: the probability of itemset <code>B</code> given itemset <code>A</code>, and the probability of itemset <code>A</code> given itemset <code>B</code>. [WCH10]</p>
<h3 id="cosine">Cosine</h3>
<p><span class="math">\[cosine(A,B)
    = \frac{P(A∪B)}{\sqrt{P(A) \times P(B)}}
    = \frac{sup(A∪B)}{\sqrt{sup(A) \times sup(B)}}
    = \sqrt{P(A|B) \times P(B|A})\]</span></p>
<p>The cosine measure can be viewed as a <em>harmonized lift</em> measure.</p>
<p>To compare the four null-invariant measures, we need <strong>imbalance ratio (IR)</strong>:</p>
<p><span class="math">\[IR(A,B) = \frac{|sup(A) − sup(B)|}{sup(A) + sup(B) − sup(A∪B)}\]</span></p>
<p>The two measures, <em>Kulc</em> and <em>IR</em>, work together, presenting a clear picture.</p>
<h2 id="measures-of-pattern-interestingness">Measures of Pattern Interestingness</h2>
<ul>
<li>For classification rules
<ul>
<li>accuracy</li>
<li>coverage</li>
</ul></li>
</ul>
<h1 id="data-preprocessing">Data Preprocessing</h1>
<p>Poor data quality may lead to poor analysis output. Some methods are susceptible to input data accuracy. Some won’t take incomplete data. Other’s will be affected by inconsistent data.</p>
<h2 id="data-cleaning">Data Cleaning</h2>
<p>Missing values</p>
<ul>
<li>Ignore</li>
<li>Fill in manually</li>
<li>Automatically fill in constants (e.g. NULL, Unknown)
<ul>
<li>Null values can be different. How to tell “I don’t remember that.” from “I do not have one.”?</li>
</ul></li>
<li>Fill in mean/median/estimation</li>
</ul>
<p>Noisy data</p>
<ul>
<li>Binning: replace values by bin mean/median/boundaries</li>
<li>Regression: replace values with regression results</li>
<li>Outlier analysis: remove outliers</li>
</ul>
<h2 id="data-integration">Data Integration</h2>
<p>Entity Identification Problem</p>
<ul>
<li>ID</li>
<li>Manual mapping</li>
</ul>
<p>Redundancy from multiple sources</p>
<ul>
<li>Correlation analysis</li>
</ul>
<p>Tuple duplication</p>
<ul>
<li>Two different IDs assigned to the same person</li>
</ul>
<p>Data value conflict</p>
<ul>
<li>different <em>unit</em></li>
<li>different <em>tax rates</em></li>
<li>GPA [0,4] vs [0,5] vs [0,100]</li>
</ul>
<h2 id="data-reduction">Data Reduction</h2>
<p><strong>Data reduction</strong> techniques can be applied to obtain a reduced representation of the data set that is much smaller in volume, yet closely maintains the integrity of the original data.</p>
<ul>
<li>Dimension reduction
<ul>
<li>Wavelet transform
<ul>
<li>fast DWT, <em>O(n)</em></li>
</ul></li>
<li>PCA<sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup></li>
<li>Attribute(feature) subset selection</li>
</ul></li>
<li>Numerosity reduction
<ul>
<li>Parametric method: use a model to estimate the data; store only the model (and outliers if necessary)
<ul>
<li>regression models</li>
</ul></li>
<li>Nonparametric method
<ul>
<li>Histogram</li>
<li>Clustering</li>
<li>Sampling</li>
<li>Data cube aggregation</li>
</ul></li>
</ul></li>
<li>Data compression
<ul>
<li>lossless</li>
<li>lossy</li>
</ul></li>
</ul>
<h2 id="data-transformation-and-data-discretization">Data Transformation and Data Discretization</h2>
<p>Strategies for data transformation include the following:</p>
<ul>
<li><strong>Smoothing</strong>, which works to remove noise from the data.</li>
<li><strong>Attribute construction</strong> (or feature construction), where new attributes are constructed and added from the given set of attributes to help the mining process.</li>
<li><strong>Aggregation</strong>, where summary or aggregation operations are applied to the data</li>
<li><strong>Normalization</strong>, where the attribute data are scaled so as to fall within a smaller range, such as −1.0 to 1.0, or 0.0 to 1.0.</li>
<li><strong>Discretization</strong>, where the raw values of a numeric attribute (e.g., age) are replaced by interval labels (e.g., 0–10, 11–20, etc.) or conceptual labels (e.g., youth, adult, senior).</li>
<li><strong>Concept hierarchy generation for nominal data</strong>, where attributes such as street can be generalized to higher-level concepts, like city or country.</li>
</ul>
<h3 id="normalization">Normalization</h3>
<ul>
<li>Min-max normalization</li>
<li>z-score normalization</li>
<li>Decimal scaling</li>
</ul>
<h3 id="discretization">Discretization</h3>
<ul>
<li>Binning</li>
<li>Histogram Analysis</li>
<li>Cluster, Decision Tree, Correlation Analysis
<ul>
<li>ChiMerge</li>
</ul></li>
</ul>
<h1 id="references">References</h1>
<p>[1]J. Han, M. Kamber, and J. Pei, <em>Data Mining: Concepts and Techniques</em>, 3rd ed. Morgan Kaufmann, 2011.</p>
<p>[2]R. Agrawal and R. Srikant, “Fast algorithms for mining association rules,” <em>Proc 20th Int Conf Very Large Data Bases</em>, 1994.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Sometimes data transformation and consolidation are performed before the data selection process, particularly in the case of data warehousing. <em>Data reduction</em> may also be performed to obtain a smaller representation of the original data without sacrificing its integrity.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>PCA tends to be better at handling sparse data, whereas wavelet transforms are more suitable for data of high dimensionality.<a href="#fnref2">↩</a></p></li>
</ol>
</section>
</body>
</html>
